# -*- coding: utf-8 -*-
"""Urdusentiment_5_CVTesting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12o8pVDUwpqe1AjRj7ISx8mtcaSiAJhE1
"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, make_scorer
from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, LeaveOneOut, train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import RidgeClassifier, Perceptron
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
# from catboost import CatBoostClassifier
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, make_scorer
from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, LeaveOneOut, train_test_split
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import RidgeClassifier, Perceptron
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
# from catboost import CatBoostClassifier
import numpy as np

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')
# %cd /content/drive/My Drive/urdu sentiments

train=pd.read_csv('trainHYBRID.csv')
test=pd.read_csv('testHYBRID.csv')
# y_train=train['class']
# X_train=train.drop(['Unnamed: 0','class'],axis=1)
# y_test=test['class']
# X_test=test.drop(['Unnamed: 0','class'],axis=1)

df = pd.concat([train,test], axis=0)

df=df.reset_index(drop=True)



y=df['class']
X=df.drop(['class','Unnamed: 0'],axis=1)

# from sklearn.preprocessing import StandardScaler
# #dataset = pd.read_csv('df.csv', sep=',')
# dataset = df
# X1 = dataset.drop(['class'],axis=1)
# Y1 =dataset['class']
# X1 = X1.to_numpy()
# Y1 = Y1.to_numpy()
# std_scale = StandardScaler().fit(X1)
# X1 = std_scale.transform(X1)
# X1 = np.nan_to_num(X1.astype('float32'))

# X=X1
# y=Y1

from sklearn.model_selection import KFold
import xgboost as xgb
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef

# Assuming you have 5 classifiers
# classifiers = [svc, ada, rf, lgbm, et]
classifiers = [LGBMClassifier(), RandomForestClassifier(), SVC(probability=True),
               xgb.XGBClassifier(), LogisticRegression()]
clf_names = ['LGBM', 'RF', 'SVC', 'XGB', 'LR']
n_folds = 5

# Loop over the classifiers
for clf, clf_name in zip(classifiers, clf_names):
    print('Classifier:', clf_name)
    # Initialize lists to store the scores for each fold
    fold_mcc = []
    fold_sp = []
    fold_sen = []
    fold_acc=[]
    fold_f1=[]
    # Split the data into n_folds
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    for train_idx, val_idx in kf.split(X):
        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]
        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]
        # Fit the classifier to the training data
        clf.fit(X_train, y_train)
        # Predict on the validation data
        y_pred = clf.predict(X_val)

        # tn, fp, fn, tp = confusion_matrix(y_val, y_val).ravel()
        # sp = tn / (tn+fp)
        f1=f1_score(y_val, y_pred)

        pr=precision_score(y_val, y_pred)

        # Compute the scores for the fold
        mcc = matthews_corrcoef(y_val, y_pred)

        acc=accuracy_score(y_val,y_pred)
        # sp = precision_score(y_val, y_pred, pos_label=0)
        sen = recall_score(y_val, y_pred, pos_label=1)

        fold_mcc.append(mcc)
        fold_sp.append(pr)
        fold_sen.append(sen)
        fold_acc.append(acc)
        fold_f1.append(f1)

    # Compute the average scores for the classifier
    avg_mcc = np.mean(fold_mcc)
    avg_sp = np.mean(fold_sp)
    avg_sen = np.mean(fold_sen)
    avg_acc = np.mean(fold_acc)
    avg_f1 = np.mean(fold_f1)

    print('MCC:', avg_mcc)
    print('PR:', avg_sp)
    print('recall:', avg_sen)
    print('Accuracy:', avg_acc)
    print('F1:', avg_f1)
    print('\n')

import numpy as np
from sklearn.model_selection import cross_val_predict, KFold
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, accuracy_score, matthews_corrcoef, precision_score, recall_score
import xgboost as xgb

# Assuming you have your feature matrix 'X' and corresponding target labels 'y'

# Define the base classifiers
svc = SVC(probability=True)
xgb_clf = xgb.XGBClassifier()

# Define the meta-classifier
lr = LogisticRegression()

# Initialize lists to store predictions and true labels
base_predictions = []
true_labels = []

# Initialize lists to store evaluation metrics for each fold
f1_scores = []
accuracies = []
mcc_scores = []
precisions = []
recalls = []

# Perform 5-fold cross-validation
kf = KFold(n_splits=5, shuffle=True)

for fold, (train_index, test_index) in enumerate(kf.split(X)):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Fit base classifiers on the training data
    svc.fit(X_train, y_train)
    xgb_clf.fit(X_train, y_train)

    # Predict probabilities using base classifiers
    svc_proba = svc.predict_proba(X_test)
    xgb_proba = xgb_clf.predict_proba(X_test)

    # Combine base classifier probabilities as features
    stacked_features = np.column_stack((svc_proba[:, 1], xgb_proba[:, 1]))

    # Store true labels
    true_labels.extend(y_test)

    # Store base classifier predictions
    base_predictions.extend(stacked_features)

    # Fit the meta-classifier on the base predictions
    lr.fit(base_predictions, true_labels)

    # Perform predictions using stacked features
    meta_predictions = lr.predict(base_predictions)

    # Compute evaluation metrics
    f1 = f1_score(true_labels, meta_predictions)
    accuracy = accuracy_score(true_labels, meta_predictions)
    mcc = matthews_corrcoef(true_labels, meta_predictions)
    precision = precision_score(true_labels, meta_predictions)
    recall = recall_score(true_labels, meta_predictions)

    # Store evaluation metrics for the fold
    f1_scores.append(f1)
    accuracies.append(accuracy)
    mcc_scores.append(mcc)
    precisions.append(precision)
    recalls.append(recall)

    # Print evaluation metrics for the fold
    print(f"Fold {fold+1}:")
    print("F1 Score:", f1)
    print("Accuracy:", accuracy)
    print("MCC:", mcc)
    print("Precision:", precision)
    print("Recall:", recall)
    print("")

# Print average evaluation metrics across all folds
print("Average Evaluation Metrics:")
print("Average F1 Score:", np.mean(f1_scores))
print("Average Accuracy:", np.mean(accuracies))
print("Average MCC:", np.mean(mcc_scores))
print("Average Precision:", np.mean(precisions))
print("Average Recall:", np.mean(recalls))

0.8805732484076433,
0.8753980891719745,
0.8694267515923567,
0.8682324840764332,
0.8660828025477707

from sklearn.ensemble import StackingClassifier
import xgboost as xgb

# Define the base classifiers
xgb_clf = xgb.XGBClassifier(random_state=42)
svc_clf = SVC(random_state=42)

# Define the meta-classifier
meta_clf = LogisticRegression(random_state=42)

# Create the stacking ensemble model
stacked_model = StackingClassifier(
    estimators=[('xgb', xgb_clf), ('svc', svc_clf)],
    final_estimator=meta_clf)



import numpy as np
from sklearn.model_selection import cross_val_predict, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score

# Assuming you have your feature matrix 'X' and corresponding target labels 'y'

# Initialize lists to store FPR and TPR for each fold
fprs = []
tprs = []
auc_scores = []

# Perform 5-fold cross-validation
kf = KFold(n_splits=5, shuffle=True)

for fold, (train_index, test_index) in enumerate(kf.split(X)):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Initialize the LR model
    # lr = LogisticRegression()

    # Fit the LR model on the training data
    stacked_model.fit(X_train, y_train)

    # Predict probabilities using the LR model
    lr_proba = stacked_model.predict_proba(X_test)[:, 1]

    # Compute FPR and TPR
    fpr, tpr, _ = roc_curve(y_test, lr_proba)

    # Compute AUC score
    auc = roc_auc_score(y_test, lr_proba)

    # Store FPR and TPR for the fold
    fprs.append(fpr)
    tprs.append(tpr)

    # Store AUC score for the fold
    auc_scores.append(auc)

    # Print evaluation metrics for the fold
    print(f"Fold {fold+1}:")
    print("False Positive Rate (FPR):", fpr)
    print("True Positive Rate (TPR):", tpr)
    print("AUC Score:", auc)
    print("")

# Print average AUC across all folds
print("Average AUC Score:", np.mean(auc_scores))
fpr = pd.DataFrame(fprs, columns=["fpr"])
tpr = pd.DataFrame(tprs, columns=["tpr"])
st1=pd.concat([fpr,tpr],axis=1)
st1.to_csv('stack.csv')

fprs

tprs

tpr=np.array(fprs).reshape(-1,1)
fpr=np.array(tprs).reshape(-1,1)

fpr = pd.DataFrame(fpr, columns=["fpr"])
tpr = pd.DataFrame(tpr, columns=["tpr"])

# fpr = pd.DataFrame({'fpr': fpr})
# fpr.to_csv('fpr.csv', index=False)

# tpr = pd.DataFrame({'tpr': tpr})
# tpr.to_csv('tpr.csv', index=False)



# # fpr = pd.DataFrame(fpr, columns=["fpr"])
# # tpr = pd.DataFrame(tprs, columns=["tpr"])
# # st1=pd.concat([fpr,tpr],axis=1)
# st1.to_csv('stack.csv')



st_auc=np.mean(auc_scores)

st_auc

# from sklearn.metrics import roc_curve, roc_auc_score

# # Compute the predicted probabilities of the meta model
# meta_prob = meta_model.predict_proba(meta_X)[:, 1]

# # Compute the fpr, tpr, and thresholds for the ROC curve
# fpr, tpr, thresholds = roc_curve(y, meta_prob)

# # Compute the AUC score
# auc_score = roc_auc_score(y, meta_prob)

# # Print the fpr, tpr, and auc score
# # print('FPR:', fpr)
# # print('TPR:', tpr)
# # print('AUC:', auc_score)

# fpr = pd.DataFrame(fpr, columns=["fpr"])
# tpr = pd.DataFrame(tpr, columns=["tpr"])
# st1=pd.concat([fpr,tpr],axis=1)
# st1.to_csv('stack.csv')

# loo = LeaveOneOut()

cv=5
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import roc_curve, auc

lgbm=cross_val_predict(LGBMClassifier(), X, y, cv=cv,method='predict_proba')
xgb=cross_val_predict(xgb.XGBClassifier(),X, y, cv=cv,method='predict_proba')
rf=cross_val_predict(RandomForestClassifier(),X, y, cv=cv,method='predict_proba')
lr=cross_val_predict(LogisticRegression(),X, y, cv=cv,method='predict_proba')
svc=cross_val_predict(SVC(probability=True),X, y, cv=cv,method='predict_proba')
# st=cross_val_predict(MLPClassifier(), X, y,cv=cv,method='predict_proba')



xgb_fpr, xgb_tpr, thresholds = roc_curve(y, xgb[:, 1])
xgb_auc = auc(xgb_fpr, xgb_tpr)


lgbm_fpr, lgbm_tpr, thresholds = roc_curve(y, lgbm[:, 1])
lgbm_auc = auc(lgbm_fpr, lgbm_tpr)

rf_fpr, rf_tpr, thresholds = roc_curve(y, rf[:, 1])
rf_auc = auc(rf_fpr, rf_tpr)

svc_fpr, svc_tpr, thresholds = roc_curve(y, svc[:, 1])
svc_auc = auc(svc_fpr, svc_tpr)

lr_fpr, lr_tpr, thresholds = roc_curve(y, lr[:, 1])
lr_auc = auc(lr_fpr, lr_tpr)

# b=pd.read_csv('stack.csv')
# strate=b.drop(['Unnamed: 0'],axis=1)

sttpr=np.array(tpr['tpr'])
stfpr=np.array(fpr['fpr'])

# st_auc = auc(stfpr, sttpr)
# bl_auc = auc(blfpr, bltpr)

# replace X1 with X_test and Y1 with y_test
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt


# # st_probs = st.predict_proba(X_test)
# st_probs = st_probs[:, 1]
# # st=st[:, 1]
# st_auc = roc_auc_score(y_test, st_probs)
# st_fpr, st_tpr, threshold = roc_curve(y_test, st_probs)


# rf_probs = rf.predict_proba(X_test)
# rf_probs = rf_probs[:, 1]
# #clf3=clf3[:,1]
# rf_auc = roc_auc_score(y_test, rf_probs)
# rf_fpr, rf_tpr, threshold = roc_curve(y_test, rf_probs)

# # lr_probs = lr.predict_proba(X_test)
# # lr_probs = lr_probs[:, 1]
# # #clf1=clf1[:,1]
# # lr_auc = roc_auc_score(y_test, lr_probs)
# # lr_fpr, lr_tpr, threshold = roc_curve(y_test, lr_probs)

# #ADA
# ada_probs = ada.predict_proba(X_test)
# ada_probs = ada_probs[:, 1]
# #clf4=clf4[:,1]
# ada_auc = roc_auc_score(y_test, ada_probs)
# ada_fpr, ada_tpr, threshold = roc_curve(y_test, ada_probs)

# #MLP
# lgbm_probs = lgbm.predict_proba(X_test)
# lgbm_probs = lgbm_probs[:, 1]
# # clf6=clf6[:,1]
# lgbm_auc = roc_auc_score(y_test, lgbm_probs)
# lgbm_fpr, lgbm_tpr, thresholdb = roc_curve(y_test, lgbm_probs)

# et_probs = etpickle.predict_proba(X_test)
# et_probs = et_probs[:, 1]
# #clf4=clf4[:,1]
# et_auc = roc_auc_score(y_test, et_probs)
# et_fpr, et_tpr, threshold = roc_curve(y_test, et_probs)

# svc_probs = svc.predict_proba(X_test)
# svc_probs = svc_probs[:, 1]
# #clf5=clf5[:,1]
# svc_auc = roc_auc_score(y_test, svc_probs)
# svc_fpr, svc_tpr, thresholde = roc_curve(y_test, svc_probs)


# final_preds = final_preds.reshape(-1, 1)
# blend_probs = blend.predict_proba(final_preds)
# blend_probs = blend_probs[:, 1]
# #clf5=clf5[:,1]
# blend_auc = roc_auc_score(y_test, blend_probs)
# blend_fpr, blend_tpr, thresholde = roc_curve(y_test, blend_probs)


#['purple', 'orange', 'brown', 'gray', 'pink']

plt.figure(figsize=(20, 10), dpi=600)
plt.plot([0, 1], [0, 1], linestyle="--", lw=2,  label="Chance", alpha=0.8)
plt.plot(rf_fpr, rf_tpr, marker='.', label='RF (auc = %0.3f)' % rf_auc)
plt.plot(lr_fpr, lr_tpr, marker='.', label='LR (auc = %0.3f)' % lr_auc)
plt.plot(svc_fpr, svc_tpr, linestyle='-', label='SVC (auc = %0.3f)' % svc_auc)
plt.plot(sttpr, stfpr, linestyle='-', label='Stacking (XGB,SVC-->LR) (auc = %0.3f)' % st_auc)
# plt.plot(blfpr, bltpr, linestyle='-', label='Blending (RF,ET,SVC,LGBM) (auc = %0.3f)' % bl_auc)
plt.plot(xgb_fpr, xgb_tpr, linestyle='-', label='XGB (auc = %0.3f)' % xgb_auc)
plt.plot(lgbm_fpr, lgbm_tpr, linestyle='-',color='black', label='LGBM (auc = %0.3f)' % lgbm_auc)




# plt.xlabel('False Positive Rate -->')
# plt.ylabel('True Positive Rate -->')

plt.legend(loc="lower right", fontsize=20, ncol=1)

plt.show()

np.mean([0.5297297297297298,
0.4818918918918919,
0.558918918918919,
0.5421621621621622,
0.6158695652173914])

lgbm=cross_val_score(LGBMClassifier(), X, y, cv=cv)

lgbm

xgb=cross_val_score(XGBClassifier(),X, y, cv=cv)
rf=cross_val_score(RandomForestClassifier(),X, y, cv=cv)
lr=cross_val_score(LogisticRegression(),X, y, cv=cv)
svc=cross_val_score(SVC(probability=True),X, y, cv=cv)
# st=cross_val_predict(MLPClassifier(), X, y,cv=cv,method='predict_proba')

xgb

rf

svc

lr

import numpy as np
#lst_accu_stratifiedlgbm5.append(np.mean(lst_accu_stratifiedlgbm5))
st10 = {'Accuracy':[0.8805732484076433,
0.8753980891719745,
0.8694267515923567,
0.8682324840764332,
0.8660828025477707],
        'folds': [1,2,3,4,5],
      'algo': 'stack'}
st10=pd.DataFrame(st10)

rf10 = {'Accuracy':[0.8383758 , 0.85748408, 0.83757962, 0.83757962, 0.83678344],
        'folds': [1,2,3,4,5],
      'algo': 'rf'}
rf10=pd.DataFrame(rf10)


lgbm10 = {'Accuracy':[0.85907643, 0.86624204, 0.85589172, 0.86146497, 0.85509554],
        'folds': [1,2,3,4,5],
      'algo': 'lgbm'}
lgbm10=pd.DataFrame(lgbm10)


xgb10 = {'Accuracy':[0.85828025, 0.86066879, 0.86066879, 0.86146497, 0.84872611],
        'folds': [1,2,3,4,5],
      'algo': 'ridge'}
xgb10=pd.DataFrame(xgb10)


svm10 = {'Accuracy':[0.85828025, 0.86464968, 0.87579618, 0.8638535 , 0.85350318],
        'folds': [1,2,3,4,5],
      'algo': 'svm'}
svm10=pd.DataFrame(svm10)


et10 = {'Accuracy':[0.84474522, 0.85907643, 0.85270701, 0.84872611, 0.83598726],
        'folds': [1,2,3,4,5],
      'algo': 'et'}
et10=pd.DataFrame(et10)

# bl10 = {'Accuracy':[0.5297297297297298,
# 0.4818918918918919,
# 0.558918918918919,
# 0.5421621621621622,
# 0.6158695652173914],
#         'folds': [1,2,3,4,5],
#       'algo': 'bl'}
# bl10=pd.DataFrame(bl10)

v10 = pd.concat([rf10, svm10,et10,xgb10,lgbm10,st10], axis=0)

# np.mean([0.588869, 0.61187661, 0.6102314, 0.6030591, 0.62187661])

from matplotlib import pyplot
import seaborn
#import mylib
a4_dims = (10, 6)
#df = mylib.load_data()
fig, ax = pyplot.subplots(figsize=a4_dims,dpi=600)
seaborn.violinplot(ax=ax, y=v10["Accuracy"], x=v10["folds"])